---
layout: post
title: Tree-Based Methods
subtitle: The fifth day
bigimg: /img/tree01.jpg
tags: [decision tree, random forest, boosting]
---

# Decision Trees

***

### Fitting Classification Trees

The `tree` library is used to construct classification and regression trees.

```r
library(tree)
```
We first use classification trees to analyze the `Carseats` data set. In these
data, `Sales` is a continuous variable, and so we begin by recoding it as a
binary variable. We use the `ifelse()` function to create a variable, called
`High`, which takes on a value of `Yes` if the `Sales` variable exceeds 8, and
takes on a value of `No` otherwise.

```r
library(ISLR)
attach(Carseats)
High <- ifelse(Sales <= 8, "No", "Yes")
```

Finally, we use the `data.frame()` function to merge `High` with the rest of
the `Carseats` data.

```r
Carseats <- data.frame(Carseats, High)
```

We now use the `tree()` function to fit a classification tree in order to predict
`High` using all variables but `Sales`. The syntax of the `tree()` function is quite
similar to that of the `lm()` function.

```r
tree.carseats <- tree(High ~ . - Sales, Carseats)
summary(tree.carseats)
```

![](/img/tree02.png)

One of the most attractive properties of trees is that they can be graphically
displayed. We use the `plot()` function to display the tree structure,
and the `text()` function to display the node labels. The argument `pretty=0`
instructs `R` to include the category names for any qualitative predictors,
rather than simply displaying a letter for each category.

```r
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```

In order to properly evaluate the performance of a classification tree on
these data, we must estimate the test error rather than simply computing
the training error. We split the observations into a training set and a test
set, build the tree using the training set, and evaluate its performance on
the test data. The `predict()` function can be used for this purpose. In the
case of a classification tree, the argument `type="class"` instructs `R` to return
the actual class prediction. This approach leads to correct predictions for
around 71.5% of the locations in the test data set.

```r
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[- train, ]
High.test = High[- train]

tree.carseats <- tree(High ~ . - Sales, Carseats, subset = train)
tree.pred <- predict(tree.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
```

![](/img/tree03.png)

```r
library(magrittr)
table(tree.pred, High.test) %>% diag %>% sum %>% `/`(., 200)
```

![](/img/tree04.png)

Next, we consider whether pruning the tree might lead to improved results.
The function `cv.tree()` performs cross-validation in order to determine
the optimal level of tree complexity; cost complexity pruning is used
in order to select a sequence of trees for consideration.We use the argument 
`FUN=prune.misclass` in order to indicate that we want the classification error
rate to guide the cross-validation and pruning process, rather than the
default for the `cv.tree()` function, which is deviance. The `cv.tree()` function
reports the number of terminal nodes of each tree considered (`size`) as
well as the corresponding error rate and the value of the cost-complexity
parameter used (`k`).

```r
set.seed(3)
(cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass))
```

![](/img/tree05.png)

Note that, despite the name, `dev` corresponds to the cross-validation error
rate in this instance. The tree with nine terminal nodes results in the lowest
cross-validation error rate, with 50 cross-validation errors.We plot the error
rate as a function of both `size` and `k`.

```r
opar <- par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")
par(opar)
```

![](/img/tree06.png)

We now apply the `prune.misclass()` function in order to prune the tree to
obtain the nine-node tree.

```r
prune.carseats <- prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```

![](/img/tree07.png)

How well does this pruned tree perform on the test data set? Once again,
we apply the `predict()` function.

```r
tree.pred <- predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
```

![](/img/tree08.png)

```r
table(tree.pred, High.test) %>% diag %>% sum %>% `/`(., 200)
```

![](/img/tree09.png)

Now 77% of the test observations are correctly classified, so not only has
the pruning process produced a more interpretable tree, but it has also
improved the classification accuracy.

### Fitting Regression Trees

Here we fit a regression tree to the `Boston` data set. First, we create a
training set, and fit the tree to the training data.

```r
library(MASS)
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)
tree.boston <- tree(medv ~ ., Boston, subset = train)
summary(tree.boston)
```

![](/img/tree10.png)

Notice that the output of `summary()` indicates that only three of the variables
have been used in constructing the tree. In the context of a regression
tree, the deviance is simply the sum of squared errors for the tree. We now
plot the tree.

```r
plot(tree.boston)
text(tree.boston, pretty = 0)
```

Now we use the `cv.tree()` function to see whether pruning the tree will
improve performance.

```r
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type = "b")
```

In this case, the most complex tree is selected by cross-validation. However,
if we wish to prune the tree, we could do so as follows, using the
`prune.tree()` function:

```r
prune.boston <- prune.tree(tree.boston, best = 5)
plot(prune.boston)
text(prune.boston, pretty = 0)
```

![](/img/tree11.png)

In keeping with the cross-validation results, we use the unpruned tree to
make predictions on the test set.

```r
yhat <- predict(tree.boston, newdata = Boston[- train, ])
boston.test <- Boston[- train, "medv"]
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test) ^ 2)
```

![](/img/tree12.png)
![](/img/tree13.png)
