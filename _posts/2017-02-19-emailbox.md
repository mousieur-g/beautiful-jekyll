---
layout: post
title: <font color=#DC143C>案例二：一个智能收件箱系统</font>
subtitle: 一个简单的排序算法
bigimg: /pics/20170219/01.jpg
tags: [<font color=#FFE4C4>邮件优先级, log加权策略, 线程活跃度</font>]
---

&emsp;&emsp;在上一个案例中，我们建立了一个垃圾邮件的识别系统。在本例中，我们将继续对这些邮件进行处理。选取文件夹`easy_ham`中的易识别的正常邮件作为本例的数据来源，并将其中的一半数据作为训练集使用，另一半即为测试集。我们试图寻找一些能够影响邮件优先级的特征进行量化处理，从而可以将邮件根据这些值进行排序，这就是我们建立一个智能收件箱的排序系统的基本原理。

### 邮件优先级的特征和量化

&emsp;&emsp;那么，什么特征适合衡量一封邮件的优先级呢？我们参照了一些相关资料，大致确定出以下五项特征作为评判一份邮件优先级的量化标准。

1. **社交特征**。如果发件人与收件人之间的交互频繁，那么可以认为来自该发件人的邮件对于收件人来说是重要的。因此在量化邮件优先级时，社交特征这方面应分配一定的权重。
2. **线程的活跃度**。我们发现，一些邮件的主题`(Subject)`是以`"Re:"`开头的，说明这是某一线程的一部分，那么该线程是一个活跃的线程，我们可以认为收件人重视这样的主题内容，故在计算邮件优先级时，应给此类邮件赋予一个相对较高的权重。
3. **发件人在线程内的活跃度**。通常情况下，对于在活跃线程内的发件人，收件人更倾向于处理其发送的邮件。因此，此项也是考量的内容之一。
4. **活跃线程的词项**。有些词汇可能频繁地出现在活跃的线程当中，很可能是收件人对这方面感兴趣。因此，当一份邮件的主题中出现这样的高频词项，那么，我们认为这封邮件对于收件人来说是具有优先的地位的。
5. **所有邮件共有词项**。为了使排序的结果尽可能地准确和科学，我们需要利用更加全面的信息，所有邮件的共有词项是一个不错的选择。

&emsp;&emsp;但是，具体量化邮件优先级的方式是什么呢？对于每一封邮件，我们都能将以上的五个特征提取出来，其值均为相应的频数。然后利用*log*加权的策略，我们可以将其频数值进行对数变换，使得彼此之间的差距不至于过大，而又存在差别，这是一个非常有用的技巧。最后，我们将所有权重相乘，最终得到该邮件优先级的量化值。可以看出，这种想法其实非常简单，但不失为一种可行的方法。

### 实现一个智能排序系统

&emsp;&emsp;首先，做好数据准备的工作，包括加载相应的程序包以及设置邮件读取的路径。

```r
# Free the memory
rm(list = ls())
gc()

# Load libraries
library(tm)
library(ggplot2)
library(plyr)
library(magrittr)
library(data.table)
library(formattable)
library(dplyr)

# Set the global paths
easyham.path <- "data/easy_ham"
```

&emsp;&emsp;自定义一些函数，获取邮件的基本信息，如邮件的接收时间、发送地址、主题以及内容。分别将其命名为`get.date`、`get.from`、`get.subject`和`get.msg`。在此之前，我们还需要定义一个`msg.full`的函数读取邮件的全部语料信息，并且在最后，定义一个`parse.email`函数汇总上述谈到的四项内容以及邮件存储的路径。

&emsp;&emsp;<font color="#DC143C">以下提供相应的R语言代码，其中的细节不做解释，后同。</font>

```r
# Simply returns the full text of a given email message
msg.full <- function(path)
{
  con <- file(path, open = "rt", encoding = "latin1")
  msg <- readLines(con)
  close(con)
  return(msg)
}

# Retuns the email address of the sender for a given
# email message
get.from <- function(msg.vec)
{
  from <- msg.vec[grepl("From: ", msg.vec)]
  from <- strsplit(from, '[":<> ]')[[1]]
  from <- from[from  != "" & from != " "]
  return(from[grepl("@", from)][1])
}

# Similar to the function from Chapter 3, this returns
# only the message body for a given email.
get.msg <- function(msg.vec)
{
  msg <- msg.vec[seq(which(msg.vec == "")[1] + 1, length(msg.vec), 1)]
  return(paste(msg, collapse = "\n"))
}

# Retuns the subject string for a given email message
get.subject <- function(msg.vec)
{
  subj <- msg.vec[grepl("Subject: ", msg.vec)]
  if(length(subj) > 0)
  {
    return(strsplit(subj, "Subject: ")[[1]][2])
  }
  else
  {
    return("")
  }
}

# Retuns the date a given email message was received
get.date <- function(msg.vec)
{
  date <- msg.vec[grepl("^Date: ", msg.vec)][1]
  date <- strsplit(date, "\\+|\\-|: ")[[1]][2]
  date <- gsub("^\\s+|\\s+$", "", date)
  return(strtrim(date, 25))
}

# This function ties all of the above helper functions together.
# It returns a vector of data containing the feature set
# used to categorize data as priority or normal HAM
parse.email <- function(path)
{
  full.msg <- msg.full(path)
      date <- get.date(full.msg)
      from <- get.from(full.msg)
      subj <- get.subject(full.msg)
       msg <- get.msg(full.msg)
  return(c(date, from, subj, msg, path))
}
```

利用`parse.email`函数解析所有易识别的正常邮件，并将结果以数据框的形式进行保存。

```r
easyham.docs  <- easyham.path %>% dir() %>% .[. != "cmds"]
easyham.parse <- lapply(easyham.docs,
                        function(p) 
                        {
                        parse.email(file.path(easyham.path, p))
                        })

allparse.df        <- easyham.parse %>% 
                      do.call(rbind, .) %>% 
                      as.data.frame()
names(allparse.df) <- c("Date", "From.Email", "Subject", 
                        "Message", "Path")
```

&emsp;&emsp;我们希望将`allparse.df`中的每条记录按照邮件接收的时间进行排序。然而，其中的时间以两种不标准的格式储存，我们需要将它们化成标准格式后才能进行排序的工作。为此，我们定义一个统一化时间格式的函数，命名为`date.converter`。

```r
# Convert date strings to POSIX for comparison. Because the emails data
# contain slightly different date format pattners we have to account 
# for this by passining them as required partmeters of the function. 
date.converter <- function(dates, pattern1, pattern2)
{
  pattern1.convert       <- strptime(dates, pattern1)
  pattern2.convert       <- strptime(dates, pattern2)
  test                   <- is.na(pattern1.convert)
  pattern1.convert[test] <- pattern2.convert[test]
  return(pattern1.convert)
}

pattern1 <- "%a, %d %b %Y %H:%M:%S"
pattern2 <- "%d %b %Y %H:%M:%S"

Sys.setlocale("LC_TIME", "English")
allparse.df$Date <- allparse.df$Date %>% 
                    date.converter(pattern1, pattern2)
```

将该数据框按时间的升序排序后，取前一半的数据作为训练集使用，剩余的则用作测试集，同时，将所有字母小写化。
```r
# Convert emails and subjects to lower-case
allparse.df$Subject    %<>% tolower()
allparse.df$From.Email %<>% tolower()

# Order the messages chronologically
priority.df <- allparse.df[with(allparse.df, order(Date)), ]

# We will use the first half of the priority.df to train our priority 
# in-box algorithm. Later, we will use the second half to test.
priority.train <- priority.df[1:(round(nrow(priority.df) / 2)), ]
```

&emsp;&emsp;接下来，我们定义一系列函数计算五大特征的各自权重。这里对代码中出现的关键的变量做简单的解释，细节部分还请读者自己琢磨。

+ *from.weight*: 社交特征
+ *senders.dt*: 发件人在线程内的活跃度
+ *thread.weights*: 线程的活跃度
+ *term.weights*: 活跃线程的词项
+ *msg.weights*: 所有邮件共有词项

```r
from.weight <- melt(with(priority.train, table(From.Email)), value.name="Freq")
from.weight %<>% arrange(Freq) %>%
                 as.data.table() %>% 
                 .[ , .(From.Email, Freq)] %>% 
                 mutate(Weight = log1p(Freq))

# This function is used to find threads within the data set.  The obvious approach
# here is to use the 're:' cue from the subject line to identify message threads.
find.threads <- function(email.df)
{
  response.threads <- strsplit(email.df$Subject, "re: ")
  is.thread <- sapply(response.threads,
                      function(subj) ifelse(subj[1] == "", TRUE, FALSE))
  threads <- response.threads[is.thread]
  senders <- email.df$From.Email[is.thread]
  threads <- sapply(threads,
                    function(t) paste(t[t != ""], collapse = "re: "))
  return(cbind(senders,threads))
}

threads.matrix <- find.threads(priority.train)

# Using the matrix of threads generated by the find.threads function this function
# creates a data from of the sender's email, the frequency of emails from that
# sender, and a log-weight for that sender based on the freqeuncy of corresponence.
email.thread <- function(threads.matrix)
{
  senders        <- threads.matrix[, 1]
  senders.freq   <- table(senders)
  senders.matrix <- cbind(names(senders.freq),
                          senders.freq,
                          log1p(senders.freq))
  senders.df <- data.frame(senders.matrix, stringsAsFactors=FALSE)
  row.names(senders.df) <- 1:nrow(senders.df)
  names(senders.df)     <- c("From.EMail", "Freq", "Weight")
  senders.df$Freq       <- as.numeric(senders.df$Freq)
  senders.df$Weight     <- as.numeric(senders.df$Weight)
  return(senders.df)
}

senders.dt<- threads.matrix %>% email.thread() %>% as.data.table()

# As an additional weight, we can enhance our notion of a thread's importance
# by measuring the time between responses for a given email.  This function
# takes a given thread and the email.df data frame to generate a weighting 
# based on this activity level.  This function returns a vector of thread
# activity, the time span of a thread, and its log-weight.
thread.counts <- function(thread, email.df)
{
  # Need to check that we are not looking at the original message in a thread, 
  # so we check the subjects against the 're:' cue.
  thread.times <- email.df$Date[which(email.df$Subject == thread |
                                        email.df$Subject == paste("re:", thread))]
  freq     <- length(thread.times)
  min.time <- min(thread.times)
  max.time <- max(thread.times)
  time.span <- as.numeric(difftime(max.time, min.time, units = "secs"))
  if(freq < 2)
  {
    return(c(NA, NA, NA))
  }
  else
  {
    trans.weight <- freq / time.span
    log.trans.weight <- 10 + log(trans.weight, base = 10)
    return(c(freq, time.span, log.trans.weight))
  }
}

# This function uses the threads.counts function to generate a weights
# for all email threads.
get.threads <- function(threads.matrix, email.df)
{
  threads       <- unique(threads.matrix[, 2])
  thread.counts <- lapply(threads,
                          function(t) thread.counts(t, email.df))
  thread.matrix <- do.call(rbind, thread.counts)
  return(cbind(threads, thread.matrix))
}

# Now, we put all of these function to work to generate a training set
# based on our thread features.
thread.weights <- get.threads(threads.matrix, priority.train)
thread.weights <- data.frame(thread.weights, stringsAsFactors = FALSE)
names(thread.weights)   <- c("Thread", "Freq", "Response", "Weight")
thread.weights$Freq     <- as.numeric(thread.weights$Freq)
thread.weights$Response <- as.numeric(thread.weights$Response)
thread.weights$Weight   <- as.numeric(thread.weights$Weight)
thread.weights %<>% data.table() %>% 
                    subset(is.na(Freq) == F)


# Similar to what we did in Chapter 3, we create a simple function to return a 
# vector of word counts.  This time, however, we keep the TDM as a free
# parameter of the function.
term.counts <- function(term.vec, control)
{
  vec.corpus <- Corpus(VectorSource(term.vec))
  vec.tdm    <- TermDocumentMatrix(vec.corpus, control = control)
  return(rowSums(as.matrix(vec.tdm)))
}

thread.terms <- term.counts(thread.weights$Thread,
                            control = list(stopwords = TRUE))
thread.terms <- names(thread.terms)

term.weights <- sapply(thread.terms,
                       function(t) t %>% 
                                   grepl(thread.weights$Thread, fixed = T) %>% 
                                   thread.weights[., Weight] %>% 
                                   mean)
term.weights <- data.table(Term = names(term.weights),
                         Weight = term.weights,
               stringsAsFactors = FALSE)

# Finally, create weighting based on frequency of terms in email. 
# Will be similar to SPAM detection, but in this case weighting
# high words that are particularly HAMMMY.

msg.terms <- term.counts(priority.train$Message,
                         control = list(stopwords = TRUE,
                                        removePunctuation = TRUE,
                                        removeNumbers = TRUE))
msg.weights <- data.table(Term = names(msg.terms),
                        Weight = log(msg.terms, base = 10),
              stringsAsFactors = FALSE) %>% subset(Weight > 0)
```
