---
layout: post
title: Tree-Based Methods
subtitle: The fifth day
bigimg: /img/tree01.jpg
tags: [decision tree, random forest, boosting]
---

# Decision Trees

***

### Fitting Classification Trees

The `tree` library is used to construct classification and regression trees.

```r
library(tree)
```
We first use classification trees to analyze the `Carseats` data set. In these
data, `Sales` is a continuous variable, and so we begin by recoding it as a
binary variable. We use the `ifelse()` function to create a variable, called
`High`, which takes on a value of `Yes` if the `Sales` variable exceeds 8, and
takes on a value of `No` otherwise.

```r
library(ISLR)
attach(Carseats)
High <- ifelse(Sales <= 8, "No", "Yes")
```

Finally, we use the `data.frame()` function to merge `High` with the rest of
the `Carseats` data.

```r
Carseats <- data.frame(Carseats, High)
```

We now use the `tree()` function to fit a classification tree in order to predict
`High` using all variables but `Sales`. The syntax of the `tree()` function is quite
similar to that of the `lm()` function.

```r
tree.carseats <- tree(High ~ . - Sales, Carseats)
summary(tree.carseats)
```

![](/img/tree02.png)

One of the most attractive properties of trees is that they can be graphically
displayed. We use the `plot()` function to display the tree structure,
and the `text()` function to display the node labels. The argument `pretty=0`
instructs `R` to include the category names for any qualitative predictors,
rather than simply displaying a letter for each category.

```r
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```

In order to properly evaluate the performance of a classification tree on
these data, we must estimate the test error rather than simply computing
the training error. We split the observations into a training set and a test
set, build the tree using the training set, and evaluate its performance on
the test data. The `predict()` function can be used for this purpose. In the
case of a classification tree, the argument `type="class"` instructs `R` to return
the actual class prediction. This approach leads to correct predictions for
around 71.5% of the locations in the test data set.

```r
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[- train, ]
High.test <- High[- train]

tree.carseats <- tree(High ~ . - Sales, Carseats, subset = train)
tree.pred <- predict(tree.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
```

![](/img/tree03.png)

```r
library(magrittr)
table(tree.pred, High.test) %>% diag %>% sum %>% `/`(., 200)
```

![](/img/tree04.png)

Next, we consider whether pruning the tree might lead to improved results.
The function `cv.tree()` performs cross-validation in order to determine
the optimal level of tree complexity; cost complexity pruning is used
in order to select a sequence of trees for consideration.We use the argument 
`FUN=prune.misclass` in order to indicate that we want the classification error
rate to guide the cross-validation and pruning process, rather than the
default for the `cv.tree()` function, which is deviance. The `cv.tree()` function
reports the number of terminal nodes of each tree considered (`size`) as
well as the corresponding error rate and the value of the cost-complexity
parameter used (`k`).

```r
set.seed(3)
(cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass))
```

![](/img/tree05.png)

Note that, despite the name, `dev` corresponds to the cross-validation error
rate in this instance. The tree with nine terminal nodes results in the lowest
cross-validation error rate, with 50 cross-validation errors.We plot the error
rate as a function of both `size` and `k`.

```r
opar <- par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")
par(opar)
```

![](/img/tree06.png)

We now apply the `prune.misclass()` function in order to prune the tree to
obtain the nine-node tree.

```r
prune.carseats <- prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```

![](/img/tree07.png)

How well does this pruned tree perform on the test data set? Once again,
we apply the `predict()` function.

```r
tree.pred <- predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
```

![](/img/tree08.png)

```r
table(tree.pred, High.test) %>% diag %>% sum %>% `/`(., 200)
```

![](/img/tree09.png)

Now 77% of the test observations are correctly classified, so not only has
the pruning process produced a more interpretable tree, but it has also
improved the classification accuracy.

### Fitting Regression Trees

Here we fit a regression tree to the `Boston` data set. First, we create a
training set, and fit the tree to the training data.

```r
library(MASS)
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)
tree.boston <- tree(medv ~ ., Boston, subset = train)
summary(tree.boston)
```

![](/img/tree10.png)

Notice that the output of `summary()` indicates that only three of the variables
have been used in constructing the tree. In the context of a regression
tree, the deviance is simply the sum of squared errors for the tree. We now
plot the tree.

```r
plot(tree.boston)
text(tree.boston, pretty = 0)
```

Now we use the `cv.tree()` function to see whether pruning the tree will
improve performance.

```r
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type = "b")
```

In this case, the most complex tree is selected by cross-validation. However,
if we wish to prune the tree, we could do so as follows, using the
`prune.tree()` function:

```r
prune.boston <- prune.tree(tree.boston, best = 5)
plot(prune.boston)
text(prune.boston, pretty = 0)
```

![](/img/tree11.png)

In keeping with the cross-validation results, we use the unpruned tree to
make predictions on the test set.

```r
yhat <- predict(tree.boston, newdata = Boston[- train, ])
boston.test <- Boston[- train, "medv"]
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test) ^ 2)
```

![](/img/tree12.png)
![](/img/tree13.png)

# Bagging and Random Forests

Here we apply bagging and random forests to the `Boston` data, using the
`randomForest` package in `R`. The exact results obtained in this section may
depend on the version of `R` and the version of the `randomForest` package
installed on your computer. Recall that bagging is simply a special case of
a random forest with m = p. Therefore, the `randomForest()` function can
be used to perform both random forests and bagging. We perform bagging
as follows:

```r
library(randomForest)
set.seed(1)
bag.boston <- randomForest(medv ~ ., data = Boston, 
                                   subset = train,
                                     mtry = 13,
                               importance = TRUE)
bag.boston
```

![](/img/tree14.png)

The argument `mtry=13` indicates that all 13 predictors should be considered
for each split of the tree â€” in other words, that bagging should be done.
How well does this bagged model perform on the test set?

```r
yhat.bag <- predict(bag.boston, newdata = Boston[- train, ])
plot(yhat.bag, boston.test)
abline(0, 1)
mean((yhat.bag - boston.test) ^ 2)
```

![](/img/tree15.png)
![](/img/tree16.png)

The test set MSE associated with the bagged regression tree is 13.47349, almost
half that obtained using using an optimally-pruned single tree. We
could change the number of trees grown by `randomForest()` using the `ntree`
argument:

```r
bag.boston <- randomForest(medv ~ ., data = Boston, 
                                   subset = train,
                                     mtry = 13, 
                                    ntree = 25)
yhat.bag = predict(bag.boston, newdata = Boston[- train, ])
mean((yhat.bag - boston.test) ^ 2)
```

![](/img/tree17.png)

Growing a random forest proceeds in exactly the same way, except that
we use a smaller value of the `mtry` argument. By default, `randomForest()`
uses p/3 variables when building a random forest of regression trees, and
sqrt(p) variables when building a random forest of classification trees. Here we
use `mtry = 6`.

```r
set.seed(1)
rf.boston <- randomForest(medv ~ ., data = Boston, 
                                  subset = train,
                                    mtry = 6,
                              importance = TRUE)
yhat.rf <- predict(rf.boston, newdata = Boston[- train, ])
mean((yhat.rf - boston.test) ^ 2)
```

![](/img/tree18.png)

The test set MSE is 11.48022; this indicates that random forests yielded an
improvement over bagging in this case.

Using the `importance()` function, we can view the importance of each
variable.

```r
importance(rf.boston)
```

![](/img/tree19.png)

Two measures of variable importance are reported. The former is based
upon the mean decrease of accuracy in predictions on the out of bag samples
when a given variable is excluded from the model. The latter is a measure
of the total decrease in node impurity that results from splits over that
variable, averaged over all trees. In the case of regression trees, the 
node impurity is measured by the training RSS, and for classification trees 
by the deviance. Plots of these importance measures can be produced 
using the `varImpPlot()` function.

```r
varImpPlot(rf.boston)
```

![](/img/tree20.png)

The results indicate that across all of the trees considered in the random
forest, the wealth level of the community (`lstat`) and the house size (`rm`)
are by far the two most important variables.

# Boosting

Here we use the `gbm` package, and within it the `gbm()` function, to fit boosted
regression trees to the `Boston` data set. We run `gbm()` with the option
`distribution="gaussian"` since this is a regression problem; if it were a binary
classification problem, we would use `distribution="bernoulli"`. The
argument `n.trees=5000` indicates that we want 5000 trees, and the option
`interaction.depth=4` limits the depth of each tree.

```r
library(gbm)
set.seed(1)
boost.boston <- gbm(medv ~ ., data = Boston[train, ], 
                      distribution = "gaussian",
                           n.trees = 5000, 
                 interaction.depth = 4)
```

The `summary()` function produces a relative influence plot and also outputs
the relative influence statistics.

```r
summary(boost.boston)
```

![](/img/tree21.png)
![](/img/tree22.png)

We see that `lstat` and `rm` are by far the most important variables. We can
also produce partial dependence plots for these two variables. These plots
illustrate the marginal effect of the selected variables on the response after
integrating out the other variables. In this case, as we might expect, median
house prices are increasing with `rm` and decreasing with `lstat`.

```r
opar <- par(mfrow = c(1, 2))
plot(boost.boston, i = "rm")
plot(boost.boston, i = "lstat")
par(opar)
```

We now use the boosted model to predict `medv` on the test set:

```r
yhat.boost <- predict(boost.boston, newdata = Boston[-train, ],
                                    n.trees = 5000)
mean((yhat.boost - boston.test) ^ 2)
```

![](/img/tree23.png)

The test MSE obtained is 11.84434; similar to the test MSE for random forests
and superior to that for bagging. If we want to, we can perform boosting
with a different value of the shrinkage parameter Î». The default
value is 0.001, but this is easily modified. Here we take Î» = 0.2.

```r
boost.boston <- gbm(medv ~ ., data = Boston[train, ], 
                      distribution = "gaussian",
                           n.trees = 5000, 
                 interaction.depth = 4, 
                         shrinkage = 0.2,
                           verbose = F)
yhat.boost <- predict(boost.boston, newdata = Boston[- train, ], 
                                    n.trees = 5000)
mean((yhat.boost - boston.test) ^ 2)
```
