---
layout: post
title: <font color=#DC143C>案例一：邮件的分类与识别</font>
subtitle: 一个粗糙的分类器应用
bigimg: /pics/20170212/01.jpg
tags: [<font color=#FFE4C4>不易识别的正常邮件, 易识别的正常邮件, 垃圾邮件</font>]
---

&emsp;&emsp;本文所使用的数据来自于`Spam Assassin`的[公开语料库](http://spamassassin.apache.org/publiccorpus/)，读者可以点击链接进行免费下载。这些文件由两套数据构成，分别用作训练集和测试集使用。而在每套数据中，均包含不易识别的正常邮件`(hard ham)`，易识别的正常邮件`(easy ham)`以及垃圾邮件`(spam)`的语料。我们的目标是建立一个简单有效的分类器，识别正常邮件和垃圾邮件。

&emsp;&emsp;那么，我们根据什么样的特征来区分正常邮件和垃圾邮件呢？根据以往的经验，词频`(word count)`信息将作为最重要的依据。在对训练集数据整理的过程中，我们会发现，某些单词在垃圾邮件中高频出现，而在正常的邮件当中很少出现甚至不出现。对于另一些单词，情况可能恰恰相反。根据经典的贝叶斯统计理论，我们可以计算出在给定一封邮件内容的情况下，其属于正常邮件和垃圾邮件的概率分别为多少。取两者中较大的概率，并将邮件归于相应的类别。这就是我们建立分类器的原理。

### 训练集数据的整理和分析

&emsp;&emsp;我们将所下载的语料保存在当前工作目录下名为`data`的文件夹里，并整理成下图的形式，以便之后对数据的提取。

<center>
<img src = "/pics/20170212/02.png">
</center>

&emsp;&emsp;完成上述的工作后，我们释放`R`的内存空间，加载本例所需的程序包，并将语料的存储路径赋值变量。

```r
rm(list = ls())
gc()

library(tm)
library(ggplot2)
library(data.table)
library(magrittr)
library(purrr)

spam.path    <- "data/spam"
easyham.path <- "data/easy_ham"
hardham.path <- "data/hard_ham"

spam2.path    <- "data/spam_2"
easyham2.path <- "data/easy_ham_2"
hardham2.path <- "data/hard_ham_2"
```

&emsp;&emsp;由于我们只关注于邮件的正文部分，所以需要设置一个能够提取邮件正文内容的函数。通常情况下，我们会发现邮件的正文总是开始于文件中的第一个空行后`(但有时候并非如此)`。根据这样的规律，我们得到`get.msg`函数。

```r
get.msg <- function(path)
          {
             con  <- file(path, open = "rt", 
                           encoding = "latin1")
             text <- readLines(con)
             close(con)
             id   <- which(text == "")
             len  <- length(text)
             if (length(id) != 0 & id[1] < len)
             {
               msg <- text[seq(id[1] + 1, len, 1)]
               return(paste(msg, collapse = "\n"))
             }
          }
```

&emsp;&emsp;现在，我们试图利用这个函数提取训练集中垃圾邮件的正文内容。由于在路径中存在`cmds`文件，其中包含的是一个很长的`unix`基本命令列表，并不是我们关注的内容。因此，在提取垃圾邮件的文件名时，应该将其剔除。然后，利用之前建立的`spam.path`变量，我们可以得到每一个垃圾邮件的完整路径。最后，结合`sapply`和`get.msg`函数，提取出训练集中所有垃圾邮件的正文内容。

```r
spam.docs <- spam.path %>% dir() %>% .[. != "cmds"]
all.spam  <- sapply(spam.docs, 
                   function(p) get.msg(file.path(spam.path, p))) 
```

这里值得注意的一点是，`spam.docs`中的某些文件可能并不符合上述有关邮件正文的规律，因此在`all.spam`中会返回`NULL`值。这里为了处理的方便，我们将其从训练集中剔除。至此，我们有了`486`份垃圾邮件的正文内容作为一部分训练样本。

```r
all.spam %<>% .[. != "NULL"]
(spam.n <- length(all.spam))
```

&emsp;&emsp;量化垃圾邮件特征词项频率的方法之一就是构造一个词项-文档矩阵`(Term Document Matrix, TDM)`。顾名思义，*TDM*矩阵的行对应在特定语料库的所有文档中抽取的词项，其列对应语料库中的所有文档，*[i, j]*位置的元素表示词项*i*在文档*j*中出现的次数。为了得到这样的矩阵，我们定义一个名为`get.tdm`的函数。

```r
get.tdm <- function(doc.vec)
           {
              doc.corpus <- Corpus(VectorSource(doc.vec))
              control <- list(stopwords = T, 
                      removePunctuation = T,
                          removeNumbers = T,
                             minDocFreq = 2)
              doc.tdm <- TermDocumentMatrix(doc.corpus, control)
              return(doc.tdm)
           }
```

关于`control`中的四个选项，这里做一些说明。

+ **stopwords = T**，意为在所有文本中移除174个最常见的英文停用词。在`R Console`中输入代码`stopwords()`即可查看停用词列表，如下图所示
+ **removePunctuation = T**，意为移除所有的标点符号
+ **removeNumbers = T**，意为移除所有的数字
+ **minDocFreq = 2**，是为了确保只有那些在文件中出现次数大于*1*的词项出现在最终的*TDM*矩阵中

![](/pics/20170212/03.png)
