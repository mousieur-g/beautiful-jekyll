---
layout: post
title: <font color=#DC143C>案例一：邮件的分类与识别</font>
subtitle: 一个粗糙的分类器应用
bigimg: /pics/20170212/01.jpg
tags: [<font color=#FFE4C4>不易识别的正常邮件, 易识别的正常邮件, 垃圾邮件</font>]
---

&emsp;&emsp;本文所使用的数据来自于`Spam Assassin`的[公开语料库](http://spamassassin.apache.org/publiccorpus/)，读者可以点击链接进行免费下载。这些文件由两套数据构成，分别用作训练集和测试集使用。而在每套数据中，均包含不易识别的正常邮件`(hard ham)`，易识别的正常邮件`(easy ham)`以及垃圾邮件`(spam)`的语料。我们的目标是建立一个简单有效的分类器，识别正常邮件和垃圾邮件。

&emsp;&emsp;那么，我们根据什么样的特征来区分正常邮件和垃圾邮件呢？根据以往的经验，词频`(word count)`信息将作为最重要的依据。在对训练集数据整理的过程中，我们会发现，某些单词在垃圾邮件中高频出现，而在正常的邮件当中很少出现甚至不出现。对于另一些单词，情况可能恰恰相反。根据经典的贝叶斯统计理论，我们可以计算出在给定一封邮件内容的情况下，其属于正常邮件和垃圾邮件的概率分别为多少。取两者中较大的概率，并将邮件归于相应的类别。这就是我们建立分类器的原理。

### 训练集数据的整理和分析

&emsp;&emsp;我们将所下载的语料保存在当前工作目录下名为`data`的文件夹里，并整理成下图的形式，以便之后对数据的提取。

<center>
<img src = "/pics/20170212/02.png">
</center>

&emsp;&emsp;完成上述的工作后，我们释放`R`的内存空间，加载本例所需的程序包，并将语料的存储路径赋值变量。

```r
rm(list = ls())
gc()

library(tm)
library(ggplot2)
library(data.table)
library(magrittr)
library(purrr)

spam.path    <- "data/spam"
easyham.path <- "data/easy_ham"
hardham.path <- "data/hard_ham"

spam2.path    <- "data/spam_2"
easyham2.path <- "data/easy_ham_2"
hardham2.path <- "data/hard_ham_2"
```

&emsp;&emsp;由于我们只关注于邮件的正文部分，所以需要设置一个能够提取邮件正文内容的函数。通常情况下，我们会发现邮件的正文总是开始于文件中的第一个空行后`(但有时候并非如此)`。根据这样的规律，我们得到`get.msg`函数。

```r
get.msg <- function(path)
          {
             con  <- file(path, open = "rt", 
                           encoding = "latin1")
             text <- readLines(con)
             close(con)
             id   <- which(text == "")
             len  <- length(text)
             if (length(id) != 0 & id[1] < len)
             {
               msg <- text[seq(id[1] + 1, len, 1)]
               return(paste(msg, collapse = "\n"))
             }
          }
```

&emsp;&emsp;现在，我们试图利用这个函数提取训练集中垃圾邮件的正文内容。由于在路径中存在`cmds`文件，其中包含的是一个很长的`unix`基本命令列表，并不是我们关注的内容。因此，在提取垃圾邮件的文件名时，应该将其剔除。然后，利用之前建立的`spam.path`变量，我们可以得到每一个垃圾邮件的完整路径。最后，结合`sapply`和`get.msg`函数，提取出训练集中所有垃圾邮件的正文内容。

```r
spam.docs <- spam.path %>% dir() %>% .[. != "cmds"]
all.spam  <- sapply(spam.docs, 
                   function(p) get.msg(file.path(spam.path, p))) 
```

这里值得注意的一点是，`spam.docs`中的某些文件可能并不符合上述有关邮件正文的规律，因此在`all.spam`中会返回`NULL`值。这里为了处理的方便，我们将其从训练集中剔除。至此，我们有了`486`份垃圾邮件的正文内容作为一部分训练样本。

```r
all.spam %<>% .[. != "NULL"]
(spam.n <- length(all.spam))
```

&emsp;&emsp;量化垃圾邮件特征词项频率的方法之一就是构造一个词项-文档矩阵`(Term Document Matrix, TDM)`。顾名思义，*TDM*矩阵的行对应在特定语料库的所有文档中抽取的词项，其列对应语料库中的所有文档，[*i*, *j*]位置的元素表示词项*i*在文档*j*中出现的次数。为了得到这样的矩阵，我们定义一个名为`get.tdm`的函数。

```r
get.tdm <- function(doc.vec)
           {
              doc.corpus <- Corpus(VectorSource(doc.vec))
              control <- list(stopwords = T, 
                      removePunctuation = T,
                          removeNumbers = T,
                             minDocFreq = 2)
              doc.tdm <- TermDocumentMatrix(doc.corpus, control)
              return(doc.tdm)
           }
```

关于其中出现的一些函数及参数，这里做一下说明。

+ *VectorSource*: 用于构建*source*对象
+ *Corpus*: 用于建立语料库对象
+ *control*: 一个选项列表，用于指定如何提取文本
+ *stopwords = T*: 在所有文本中移除*174*个最常见的英文停用词。在`R Console`中输入代码`stopwords()`即可查看停用词列表，如下图所示
+ *minDocFreq = 2*: 确保只有那些在文件中出现次数大于*1*的词项出现在最终的$TDM$矩阵中


![](/pics/20170212/03.png)

&emsp;&emsp;利用`get.tdm`函数，我们可以将`all.spam`中的所有正文内容转化为*TDM*矩阵，并整理出训练样本中垃圾邮件的词频信息。

```r
spam.tdm        <- get.tdm(all.spam)
spam.matrix     <- as.matrix(spam.tdm)
spam.counts     <- rowSums(spam.matrix)
spam.density    <- spam.counts / sum(spam.counts)
spam.occurrence <- sapply(1:nrow(spam.matrix),
                         function(p) 
                         {
                           sum(spam.matrix[p, ] != 0) / spam.n
                         })

spam.dt <- data.table(term = names(spam.counts),
                 frequency = spam.counts,
                   density = spam.density,
                occurrence = spam.occurrence)
```
这里的`density`和`occurrence`都是统计词项出现的频率，然而它们是不同的。前者表示该词项出现的总次数占所有词项出现次数的百分比，后者表示出现该词项的邮件数占总邮件数的百分比。我们之后计算条件概率时，所使用的是`occurrence`，而并非`density`,就是基于这样的解释。将以上数据按`occurrence`的降序排列，查看前十个出现频率较高的词项。

```r
setorder(spam.dt, -occurrence)[]
head(spam.dt, 10)
```

![](/pics/20170212/04.png)

为了使结果更为直观，我们进行数据的可视化。

```r
new_theme <- theme_update(
             panel.background = element_rect(fill = NA),
           panel.grid.major.y = element_line(color = NA),
             panel.grid.major = element_line(colour = "light pink",
                                           linetype = "dotted",
                                               size = 1),
             panel.grid.minor = element_line(colour = "light green",
                                           linetype = "dotted",
                                               size = 1),
                  axis.text.x = element_text(colour = "black"),
                  axis.text.y = element_text(colour = "black", 
                                              hjust = 1),
                 axis.title.x = element_text(colour = "black",
                                               face = "bold"),
                 axis.title.y = element_text(colour = "black",
                                               face = "bold",
                                              angle = 0,
                                              vjust = 0.95),
                  axis.line.x = element_line(size = 1,
                                           colour = "black"),
                  axis.line.y = element_line(size = 1,
                                           colour = "black")
                        )

p <- head(spam.dt, 10) %>% 
     ggplot(aes(term, occurrence, fill = term)) 
     
p +  geom_bar(aes(reorder(term, occurrence)), stat = "identity",
                                       show.legend = F) + 
     geom_text(aes(label = round(occurrence, 3), hjust = 1.5)) + 
     coord_flip(ylim = c(0.2, 0.6))
```

![](/pics/20170212/05.png)

&emsp;&emsp;对训练集中同等数量的易识别邮件做类似处理，具体步骤就不再重复说明。代码和结果如下。

```r
easyham.docs <- easyham.path %>% dir() %>% .[. != "cmds"]
all.easyham  <- sapply(easyham.docs[1:length(spam.docs)],
                      function(p) get.msg(file.path(easyham.path, p)))

sum(all.easyham == "NULL")
easyham.n <- spam.n

easyham.tdm        <- get.tdm(all.easyham)
easyham.matrix     <- as.matrix(easyham.tdm)
easyham.counts     <- rowSums(easyham.matrix)
easyham.density    <- easyham.counts / sum(easyham.counts)
easyham.occurrence <- sapply(1:nrow(easyham.matrix),
                             function(p) 
                             {
                              sum(easyham.matrix[p, ] != 0) / easyham.n
                             })

easyham.dt <- data.table(term = names(easyham.counts),
                    frequency = easyham.counts,
                      density = easyham.density,
                   occurrence = easyham.occurrence)

setorder(easyham.dt, -occurrence)[]
head(easyham.dt, 10)

p <- head(easyham.dt, 10) %>% 
     ggplot(aes(term, occurrence, fill = term)) 
     
p +  geom_bar(aes(reorder(term, occurrence)), stat = "identity",
                                       show.legend = F) + 
     geom_text(aes(label = round(occurrence, 3), hjust = 1.5)) + 
     coord_flip(ylim = c(0.2, 0.4))

theme_set(new_theme)
```

![](/pics/20170212/06.png)

![](/pics/20170212/07.png)

&emsp;&emsp;<font color="#DC143C">为了简化问题，这里只将上述所得的垃圾邮件和易识别的邮件的词频信息作为训练集使用，而在实际操作时，也应考虑一部分不易识别的邮件的词频。</font>

&emsp;&emsp;根据贝叶斯公式，我们有以下几个等式成立：
 
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>

$$
P(Y \in Spam \mid X) = \frac{P(Y \in Spam, X)}{P(X)} = \frac{P(Y \in Spam) \times P(X \mid Y \in Spam)}{P(X)} \n
P(Y \in Ham \mid X) = \frac{P(Y \in Ham, X)}{P(X)} = \frac{P(Y \in Hpam) \times P(X \mid Y \in Hpam)}{P(X)}
$$

 因此，有如下的等价关系：
 
 $$
 P(Y \in Spam \mid X) > P(Y \in Ham \mid X) \Leftrightarrow \n
 P(Y \in Spam) \times P(X \mid Y \in Spam) > P(Y \in Hpam) \times P(X \mid Y \in Hpam)
 $$
